{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 30188.csv\n",
      "     Base Page ID Base Page Title                          Base Page URL  \\\n",
      "0           30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "1           30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "2           30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "3           30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "4           30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "..            ...             ...                                    ...   \n",
      "489         30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "490         30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "491         30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "492         30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "493         30188         Tunisia  https://en.wikipedia.org/?curid=30188   \n",
      "\n",
      "                                 Link Title  Link Page ID  \n",
      "0                                       .tn       1238233  \n",
      "1                        12th meridian east      20941111  \n",
      "2                 1987 Tunisian coup d'Ã©tat      49363146  \n",
      "3    2005 World Men's Handball Championship       1463977  \n",
      "4                      2008 Summer Olympics         77745  \n",
      "..                                      ...           ...  \n",
      "489                                Kairouan        433187  \n",
      "490                    Kairouan Governorate       4374957  \n",
      "491                              Kais Saied      61361599  \n",
      "492                      Head of government         37570  \n",
      "493      International Festival of Carthage      43236489  \n",
      "\n",
      "[494 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "# Function to fetch base page information\n",
    "def fetch_wikipedia_page_info(title):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"info\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"inprop\": \"url|id\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    page_info = {}\n",
    "    for page_id, page in pages.items():\n",
    "        page_info['id'] = page['pageid']\n",
    "        page_info['title'] = page['title']\n",
    "        page_info['url'] = f\"https://en.wikipedia.org/?curid={page['pageid']}\"\n",
    "    return page_info\n",
    "\n",
    "# Function to fetch links from a page\n",
    "def fetch_wikipedia_links(title):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"links\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"pllimit\": \"max\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    links = []\n",
    "    for page_id, page in pages.items():\n",
    "        link_list = page.get('links', [])\n",
    "        for link in link_list:\n",
    "            links.append({'title': link['title']})\n",
    "    return links\n",
    "\n",
    "# Function to fetch page ID for a given title\n",
    "def fetch_page_id(title):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"info\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"inprop\": \"id\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    page_info = {}\n",
    "    for page_id, page in pages.items():\n",
    "        page_info['title'] = page['title']\n",
    "        page_info['pageid'] = page.get('pageid', None)\n",
    "    return page_info\n",
    "\n",
    "# Function to fetch page IDs for all links in parallel\n",
    "def fetch_all_link_ids(links):\n",
    "    link_ids = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_link = {executor.submit(fetch_page_id, link['title']): link for link in links}\n",
    "        for future in concurrent.futures.as_completed(future_to_link):\n",
    "            link = future_to_link[future]\n",
    "            try:\n",
    "                page_info = future.result()\n",
    "                if page_info['pageid']:\n",
    "                    link_ids.append({'title': page_info['title'], 'pageid': page_info['pageid']})\n",
    "            except Exception as exc:\n",
    "                print(f'Error fetching page ID for {link[\"title\"]}: {exc}')\n",
    "    return link_ids\n",
    "\n",
    "# Function to save data to CSV using pandas\n",
    "def save_to_csv(base_page_info, links_with_ids):\n",
    "    base_title = base_page_info['id']\n",
    "    filename = str(base_page_info['id'])+\".csv\"\n",
    "\n",
    "    data = []\n",
    "    for link in links_with_ids:\n",
    "        data.append([\n",
    "            base_page_info['id'],\n",
    "            base_page_info['title'],\n",
    "            base_page_info['url'],\n",
    "            link['title'],\n",
    "            link['pageid']\n",
    "        ])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Base Page ID', 'Base Page Title', 'Base Page URL', 'Link Title', 'Link Page ID'])\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Function to read data from CSV using pandas\n",
    "def read_from_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    base_page_title = \"Tunisia\"\n",
    "    base_page_info = fetch_wikipedia_page_info(base_page_title)\n",
    "    links = fetch_wikipedia_links(base_page_title)\n",
    "    links_with_ids = fetch_all_link_ids(links)\n",
    "    save_to_csv(base_page_info, links_with_ids)\n",
    "\n",
    "    # Optionally read and print the CSV data\n",
    "    df = read_from_csv(str(base_page_info['id'])+\".csv\")\n",
    "    print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
