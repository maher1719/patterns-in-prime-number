# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s9yfAX2kWktWF8TjfbUx6eHWqS4w4Mkm

Setup and import
"""

import numpy as np
from scipy.optimize import fsolve
from sympy import primepi
import matplotlib.pyplot as plt
import mpmath
import csv
import scipy
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from scipy.optimize import leastsq
from sklearn.linear_model import LinearRegression

def equation(x, a, pi_a):
    argument = a - a / (x * np.pi / 2)
    if argument <= 0:
        return np.nan
    result = pi_a - a / np.log(argument)
    return float(result)

# Define the range and step size for a
a_start = 1000000
a_end = 100000000
a_step = 50000

# Generate values for a
a_values = np.arange(a_start, a_end + a_step, a_step)

# Arrays to hold the results
x_solutions = []
f_values = []
abs_li_minus_pi_values = []
abs_f_minus_pi_values = []

# Compute x for each value of a
for a in a_values:
    pi_a = primepi(a)
    x_initial_guess = 1
    x_solution, = fsolve(equation, x_initial_guess, args=(a, pi_a))
    print(a, x_solution)
    x_solutions.append(x_solution)

    # Calculate the modified f(x) using the solution x
    if x_solution > 0:  # Ensure x_solution is positive
        f_x = a / np.log(a - a / (x_solution * np.pi / 2))
    else:
        f_x = np.nan

    f_values.append(f_x)

    # Calculate Li(x) using mpmath
    li_x = mpmath.li(a)

    abs_li_minus_pi = np.abs(li_x - pi_a)
    abs_f_minus_pi = np.abs(f_x - pi_a)

    abs_li_minus_pi_values.append(abs_li_minus_pi)
    abs_f_minus_pi_values.append(abs_f_minus_pi)

# Perform linear regression on x_solutions based on a_values
slope, intercept = np.polyfit(a_values, x_solutions, 1)
print(f"Linear regression equation: x = {slope:.15f} * a + {intercept:.15f}")

def logarithmic_regression(x, a):
    def model(params, x):
        c, d = params
        return c + d * np.log(x)

    def objective(params, x, y):
        return model(params, x) - y

    # Initial guess for parameters (c, d)
    initial_params = [0, 1]

    # Solve for parameters using nonlinear least squares
    params, _ = scipy.optimize.leastsq(objective, initial_params, args=(x, a))

    return params

# Perform logarithmic regression on x_solutions based on a_values
log_params = logarithmic_regression(a_values, np.array(x_solutions))
log_predictions = log_params[0] + log_params[1] * np.log(a_values)
log_params[1]
print(f"Logarithmic regression equation: x = {log_params[0]:.15f} + {log_params[1]:.15f} * log(a)")

"""Compute R^2 for log predictions"""

r2_log = r2_score(x_solutions, log_predictions)
print(f"R-squared for Logarithmic Regression: {r2_log:.12f}")

"""Compute Mean Squared Error for the logarithmic regression"""

# Compute Mean Squared Error for the logarithmic regression
mse_log = mean_squared_error(x_solutions, log_predictions)
print(f"Mean Squared Error for Logarithmic Regression: {mse_log:.12f}")

# Compute Root Mean Squared Error for the logarithmic regression
rmse_log = np.sqrt(mse_log)
print(f"Root Mean Squared Error for Logarithmic Regression: {rmse_log:.12f}")

def transformed_function(a, slope, intercept):
    x = slope * a + intercept
    if a - (0.6366197724 * a) / x <= 0:
        return np.nan
    pi=np.pi
    return primepi(a) - (a / np.log(a - (0.6366197724 * a) / x))

transformed_values = [transformed_function(a, slope, intercept) for a in a_values]

# Save the results to a CSV file
csv_filename = "results2.csv"
with open(csv_filename, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['a', 'x_solution', 'f(x)', '|Li(x) - pi(x)|', '|f(x) - pi(x)|'])
    for a, x_solution, f_x, li_minus_pi, f_minus_pi in zip(a_values, x_solutions, f_values, abs_li_minus_pi_values, abs_f_minus_pi_values):
        writer.writerow([a, x_solution, f_x, li_minus_pi, f_minus_pi])

print(f"Results saved to {csv_filename}")

def adjusted_r2(r2, n, p):
    return 1 - (1 - r2) * (n - 1) / (n - p - 1)

# Number of observations and predictors
n = len(a_values)
p = 1  # Only one predictor in logarithmic regression

adj_r2_log = adjusted_r2(r2_log, n, p)
print(f"Adjusted R-squared for Logarithmic Regression: {adj_r2_log:.4f}")
from scipy import stats
residuals_log = np.array(x_solutions) - np.array(log_predictions)
# Compute F-statistic and p-value
ss_res = np.sum(residuals_log ** 2)
ss_tot = np.sum((np.array(x_solutions) - np.mean(x_solutions)) ** 2)
f_statistic = (ss_tot - ss_res) / p / (ss_res / (n - p - 1))
p_value = 1 - stats.f.cdf(f_statistic, p, n - p - 1)
print(f"Logarithmic regression equation: x = {log_params[0]:.15f} + {log_params[1]:.15f} * log(a)")

"""Compute R^2 for log predictions"""

r2_log = r2_score(x_solutions, log_predictions)
print(f"R-squared for Logarithmic Regression: {r2_log:.12f}")

"""Compute Mean Squared Error for the logarithmic regression"""

# Compute Mean Squared Error for the logarithmic regression
mse_log = mean_squared_error(x_solutions, log_predictions)
print(f"Mean Squared Error for Logarithmic Regression: {mse_log:.12f}")

# Compute Root Mean Squared Error for the logarithmic regression
rmse_log = np.sqrt(mse_log)
print(f"Root Mean Squared Error for Logarithmic Regression: {rmse_log:.12f}")

print(f"R-squared for Logarithmic Regression: {r2_log:.32}")
print(f"Mean Squared Error for Logarithmic Regression: {mse_log:.32}")
print(f"Root Mean Squared Error for Logarithmic Regression: {rmse_log:.32}")
print(f"F-statistic for Logarithmic Regression: {f_statistic:.32}")
print(f"P-value for F-statistic: {p_value:.32f}")

# Plot the linear regression line
plt.figure(figsize=(12, 6))
plt.plot(a_values, x_solutions, marker='o', linestyle='-', color='b', label='x solutions')
plt.plot(a_values, slope * a_values + intercept, linestyle='--', color='g', label='Linear fit')
plt.plot(a_values, log_predictions, linestyle='-.', color='r', label='Logarithmic fit')
plt.xlabel('a')
plt.ylabel('x')
plt.title('Plot of x vs. a with Linear and Logarithmic Fits')
plt.legend()
plt.grid(True)


# Compute residuals
residuals_log = np.array(x_solutions) - np.array(log_predictions)

# Plot residuals
plt.figure(figsize=(12, 6))
plt.scatter(a_values, residuals_log, marker='o',linestyle='-', color='r', label='Residuals')
plt.axhline(y=0, color='b', linestyle='--')
plt.xlabel('a')
plt.ylabel('Residuals')
plt.title('Residuals Plot for Logarithmic Regression')
plt.legend()
plt.grid(True)



# Plotting |Li(x) - pi(x)| and |f(x) - pi(x)| on the same graph
plt.figure(figsize=(12, 6))
plt.plot(a_values, abs_li_minus_pi_values, marker='o', linestyle='-', color='r', label='|Li(x) - pi(x)|')
plt.plot(a_values, abs_f_minus_pi_values, marker='o', linestyle='-', color='g', label='|f(x) - pi(x)|')
plt.xlabel('a')
plt.ylabel('Absolute Differences')
plt.title('Comparison of |Li(x) - pi(x)| and |f(x) - pi(x)|')
plt.legend()
plt.grid(True)


# Plot the transformed function
plt.figure(figsize=(12, 6))
plt.plot(a_values, transformed_values, marker='o', linestyle='-', color='m', label='Transformed function')
plt.xlabel('a')
plt.ylabel('Transformed Value')
plt.title('Plot of Transformed Function')
plt.grid(True)
plt.legend()


import mpmath

# Define the logarithmic integral function
def li(x):
    return mpmath.li(x)

# Define the modified function based on your previous code

# Arrays to hold the results
li_errors = []
mod_errors = []
m_error=np.array([])

def objective2(params, x, y):
    a, b = params
    return y-y+1

def logarithmic_regression2(x, y):
    # Initial guess for the parameters (a and b)
    initial_params = [1.0, 1.0]
    
    # Perform the least squares optimization
    params, _ = leastsq(objective2, initial_params, args=(x, y))
    
    return params


def modified_function2(a, rg):
    argument = a - a / (rg * np.pi / 2)
    if argument <= 0:
        return np.nan  # Returning np.nan to handle later
    return a / np.log(argument)
# Compute errors for each value of x
for a, x in zip(a_values, x_solutions):
    pi_x = primepi(a)
    li_x = li(a)
    rg = log_params[0] + log_params[1] * np.log(x)
    mod_x = modified_function2(a, rg)
    print(a,x,rg,mod_x)
    li_errors.append(abs(pi_x - li_x))
    mod_errors.append(abs(pi_x - mod_x))



# Transform a_values to log scale and reshape for sklearn
log_a_values=a_values.reshape(-1, 1)
# Ensure that abs(f(x) - pi(x)) is in a numpy array and reshaped for sklearn
y_values = np.array(mod_errors).reshape(-1, 1)

# Create a LinearRegression model
lin_reg = LinearRegression()

# Fit the model to the log-transformed data
lin_reg.fit(log_a_values, y_values)

# Get the predictions
y_pred = lin_reg.predict(log_a_values)

# Check for NaN or Inf in y_pred
if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):
    raise ValueError("y_pred contains NaN or Inf values")

# Flatten y_values and y_pred for metrics calculations
y_values = np.ravel(y_values)
y_pred = np.ravel(y_pred)

# Calculate R-squared
r2 = r2_score(y_values, y_pred)
print(f"R-squared: {r2}")

# Calculate Mean Squared Error
mse = mean_squared_error(y_values, y_pred)
print(f"Mean Squared Error: {mse}")

# Calculate Root Mean Squared Error
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error: {rmse}")

intercept = lin_reg.intercept_[0]
slope = lin_reg.coef_[0][0]
print(f"Linear Regression Equation: y = {intercept:.10f} + {slope:.10f} * x")

# Plot the results
plt.figure(figsize=(12, 6))
plt.scatter(log_a_values, y_values, color='blue', label='Original Data')
plt.plot(log_a_values, y_pred, color='red', label='Linear Fit')
plt.xlabel('log(a_values)')
plt.ylabel('abs(f(x) - pi(x))')
plt.title('Linear Regression of abs(f(x) - pi(x))')
plt.legend()
plt.grid(True)



# Create the plot"""
plt.figure(figsize=(12, 8))
plt.semilogy(a_values, li_errors, label='|π(a) - Li(a)|')
plt.semilogy(a_values, mod_errors, label='|π(a) - Modified function|')


plt.title('Comparison of Absolute Errors: Li(a) vs Modified Function')
plt.xlabel('a')
plt.ylabel('Absolute Error')
plt.legend(title='Error Type')
plt.grid(True)

# Save the plot as a PNG file
plt.savefig('error_comparison_plot.png', dpi=300, bbox_inches='tight')

# Display the plot (optional, if you're running this in an environment that can show plots)
plt.show()