{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expanding question with Ollama...\n",
      "Ollama Question Expansion Response: [\n",
      "  \"what is tunisia?\",\n",
      "  \"What is Tunisian culture like today?\",\n",
      "  \"How did Tunisia become an island?\",\n",
      "  \"When was ancient Carthage founded?\"\n",
      "]\n",
      "Expanded Questions (Parsed as JSON array): ['what is tunisia?', 'What is Tunisian culture like today?', 'How did Tunisia become an island?', 'When was ancient Carthage founded?']\n",
      "\n",
      "--- Searching and Scraping for Expanded Question: 'what is tunisia?' ---\n",
      "Warning: Captcha page detected for query: 'what is tunisia?'. Scraping stopped.\n",
      "Captcha detected, stopping scraping.\n",
      "Error: Captcha detected. Please try again later.\n"
     ]
    }
   ],
   "source": [
    "# your_ollama_script.py\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import json\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from newspaper import Article\n",
    "\n",
    "# --- Configuration ---\n",
    "OLLAMA_MODEL = \"llama3.2\" # Or \"llama3.2\" if that's your model name in Ollama\n",
    "SEARCH_ENGINE = \"brave\"  # Now ONLY Brave Search\n",
    "SEARCH_API_KEY = \"YOUR_SERP_API_KEY\" # Not used for direct Brave scraping\n",
    "\n",
    "# --- Search Engine Functions ---\n",
    "async def search_brave_direct(query):\n",
    "    \"\"\"Directly scrapes Brave search results using Playwright Async API.\"\"\"\n",
    "    search_query = urllib.parse.quote_plus(query)\n",
    "    url = f\"https://search.brave.com/search?q={search_query}&source=web\"\n",
    "\n",
    "    links = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        try:\n",
    "            await page.goto(url, timeout=30000)\n",
    "\n",
    "            # --- Captcha Detection Logic ---\n",
    "            page_title = await page.title()  # AWAIT page.title() to get the string!\n",
    "            if \"captcha\" in page_title.lower() or \"robot verification\" in page_title.lower():\n",
    "                print(f\"Warning: Captcha page detected for query: '{query}'. Scraping stopped.\")\n",
    "                await browser.close()\n",
    "                return None  # Return None to signal captcha\n",
    "\n",
    "            # --- UPDATED SELECTOR for Brave Search result links ---\n",
    "            link_elements = await page.locator('div.snippet  a.heading-serpresult').all() # Changed selector here!\n",
    "\n",
    "            if not link_elements: # Check if link_elements is empty\n",
    "                print(f\"Warning: No search result link elements found on Brave Search page for query: '{query}'.\")\n",
    "                return [] # Return empty list if no links found\n",
    "\n",
    "            for link_element in link_elements:\n",
    "                href = await link_element.get_attribute('href')\n",
    "                if href and href.startswith(\"http\") and \"brave.com\" not in href:\n",
    "                    links.append(href)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Playwright navigation or scraping Brave Search: {e}\")\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "    if not links: # Check if links list is empty after processing\n",
    "        print(f\"Warning: No external links extracted from Brave Search results for query: '{query}'.\")\n",
    "\n",
    "    return links\n",
    "async def get_search_links(query, search_engine=SEARCH_ENGINE, search_api_key=SEARCH_API_KEY):\n",
    "    \"\"\"Orchestrates search using Brave search engine, handles captcha detection.\"\"\"\n",
    "    if search_engine == \"brave\":\n",
    "        search_results = await search_brave_direct(query) # Get result (can be links or None)\n",
    "        if search_results is None: # Check for None (captcha signal)\n",
    "            return None # Propagate None to indicate captcha\n",
    "        return search_results\n",
    "    else:\n",
    "        print(f\"Warning: Search engine '{search_engine}' is not 'brave'. Defaulting to direct Brave search.\")\n",
    "        search_results = await search_brave_direct(query) # Get result (can be links or None)\n",
    "        if search_results is None: # Check for None (captcha signal)\n",
    "            return None # Propagate None to indicate captcha\n",
    "        return search_results\n",
    "\n",
    "\n",
    "# --- Website Content Scraping Function ---\n",
    "\n",
    "async def scrape_website_content(url):\n",
    "    \"\"\"Scrapes text content from a website URL using Playwright Async API, newspaper3k, and BeautifulSoup as fallback.\"\"\"\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch()\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url, timeout=30000)\n",
    "            html_content = await page.content()\n",
    "\n",
    "            try:\n",
    "                # --- Attempt to use newspaper3k for better article extraction ---\n",
    "                article = Article(url) # URL is still needed by newspaper\n",
    "                article.set_html(html_content) # Pass HTML from Playwright\n",
    "                article.parse()\n",
    "                text_content = article.text\n",
    "                if text_content.strip(): # Check if newspaper3k extracted content\n",
    "                    print(f\"Successfully extracted content using newspaper3k from: {url}\")\n",
    "                    return text_content\n",
    "                else:\n",
    "                    print(f\"newspaper3k failed to extract meaningful content, falling back to basic parsing for: {url}\")\n",
    "\n",
    "            except Exception as newspaper_err:\n",
    "                print(f\"Error using newspaper3k for {url}: {newspaper_err}. Falling back to basic parsing.\")\n",
    "\n",
    "            # --- Fallback to basic BeautifulSoup parsing (if newspaper3k fails or extracts nothing) ---\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            text_content = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "            return text_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url} with Playwright: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# --- Ollama Query Function ---\n",
    "\n",
    "def query_ollama(prompt, model_name=OLLAMA_MODEL):\n",
    "    \"\"\"Queries a local Ollama model.\"\"\"\n",
    "    try:\n",
    "        import ollama\n",
    "        response = ollama.chat(model=model_name, messages=[{'role': 'user', 'content': prompt}])\n",
    "        return response['message']['content']\n",
    "    except ImportError:\n",
    "        import requests\n",
    "        api_url = \"http://localhost:11434/api/chat\"\n",
    "        data = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        }\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(api_url, json=data, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying Ollama: {e}\")\n",
    "        return \"Error communicating with Ollama.\"\n",
    "\n",
    "# --- Question Expansion Function ---\n",
    "def expand_question_with_ollama(question):\n",
    "    \"\"\"Expands the initial question using Ollama to generate related deeper questions.\n",
    "       Now requests JSON array format and max 6-word questions.\"\"\"\n",
    "    expansion_prompt = f\"\"\"Please expand on the question: \"{question}\".\n",
    "Generate 5 additional questions that are meaningful, delve deeper into the subject incrementally, and are related to the original question.\n",
    "Ensure that each expanded question is a maximum of 6 words long, while preserving its meaning and relevance.\n",
    "Return the original question and the 5 expanded questions as a JSON array of strings.\n",
    "\n",
    "Example JSON output:\n",
    "[\n",
    "  \"{question}\",\n",
    "  \"Question 1 (max 6 words)\",\n",
    "  \"Question 2 (max 6 words)\",\n",
    "  \"Question 3 (max 6 words)\",\n",
    "  \"Question 4 (max 6 words)\",\n",
    "  \"Question 5 (max 6 words)\"\n",
    "]\n",
    "\n",
    "Only output the JSON array.\"\"\" # Updated prompt with word limit and array format\n",
    "    print(\"\\nExpanding question with Ollama...\")\n",
    "    try:\n",
    "        json_response = query_ollama(expansion_prompt)\n",
    "        print(f\"Ollama Question Expansion Response: {json_response}\") # Print raw response for debugging\n",
    "\n",
    "        # --- Attempt to parse JSON array ---\n",
    "        try:\n",
    "            expanded_questions = json.loads(json_response) # Directly parse as JSON array\n",
    "            if isinstance(expanded_questions, list): # Verify it's a list\n",
    "                print(f\"Expanded Questions (Parsed as JSON array): {expanded_questions}\")\n",
    "                return expanded_questions\n",
    "            else:\n",
    "                print(f\"Warning: Parsed JSON, but not a list (array). Unexpected format. Raw response: {json_response}\")\n",
    "                return [question] # Fallback to original question\n",
    "\n",
    "        except json.JSONDecodeError as json_err:\n",
    "            print(f\"JSON Decode Error: {json_err}. Raw response was: {json_response}. Falling back to original question.\")\n",
    "            return [question] # Fallback to original question\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during question expansion with Ollama: {e}. Falling back to original question.\")\n",
    "        return [question] # Fallback to original question\n",
    "\n",
    "\n",
    "# --- Main Function ---\n",
    "\n",
    "async def main(user_question): # Take user_question as argument\n",
    "    # user_question = input(\"Ask me anything: \") # No input here for web interface\n",
    "\n",
    "    # --- Expand the question using Ollama ---\n",
    "    expanded_questions = expand_question_with_ollama(user_question)\n",
    "\n",
    "    all_scraped_content = \"\" # Accumulate content from all expanded questions\n",
    "    scraped_webpages = [] # List to track scraped webpages\n",
    "\n",
    "    for q in expanded_questions:\n",
    "        print(f\"\\n--- Searching and Scraping for Expanded Question: '{q}' ---\")\n",
    "        search_results_links = await get_search_links(q)\n",
    "\n",
    "        if search_results_links is None: # Captcha detected, stop scraping\n",
    "            print(\"Captcha detected, stopping scraping.\")\n",
    "            return None, \"Captcha detected. Please try again later.\", [] # Return None for content, captcha message, empty webpages\n",
    "\n",
    "        if not search_results_links:\n",
    "            print(f\"No search results found for expanded question: '{q}'.\")\n",
    "            continue # Go to the next expanded question\n",
    "\n",
    "        print(f\"Found search results for expanded question: '{q}'. Scraping content...\")\n",
    "        for link in search_results_links[:2]: # Scrape content from top 2 links for each expanded question (adjust as needed)\n",
    "            print(f\"Scraping: {link}\")\n",
    "            content = await scrape_website_content(link)\n",
    "            all_scraped_content += f\"\\n\\n---\\n\\nURL for Question '{q}': {link}\\nContent:\\n{content}\"\n",
    "            scraped_webpages.append({\"question\": q, \"url\": link}) # Track scraped webpages\n",
    "\n",
    "    # --- Prepare Prompt for LLM with ALL scraped content and ALL questions ---\n",
    "    questions_for_prompt = \"\"\n",
    "    for i, q in enumerate(expanded_questions):\n",
    "        questions_for_prompt += f\"{i+1}. {q}\\n\"\n",
    "\n",
    "    final_prompt_for_llm = f\"\"\"Answer the following question based on the information provided below.\n",
    "Consider ALL the questions listed below when formulating your answer. And give long answer as you could.\n",
    "If the information is not in the context, or not relevant to ANY of the questions, say \"I cannot find the answer in the provided context.\"\n",
    "\n",
    "Questions to consider:\n",
    "--- Questions ---\n",
    "{questions_for_prompt}\n",
    "--- End Questions ---\n",
    "\n",
    "--- Context ---\n",
    "{all_scraped_content}\n",
    "--- End Context ---\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    print(\"\\nSending to Ollama for final answer...\")\n",
    "    llm_response = query_ollama(final_prompt_for_llm)\n",
    "    print(\"\\n--- Final Response from Ollama ---\")\n",
    "    print(llm_response)\n",
    "\n",
    "    return llm_response, None, scraped_webpages # Return response, no error message, scraped webpages\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input_question = input(\"Ask me anything: \") # Keep input for direct script run\n",
    "    response, error_msg, webpages = asyncio.run(main(user_input_question))\n",
    "    if error_msg:\n",
    "        print(f\"Error: {error_msg}\")\n",
    "    elif response:\n",
    "        print(\"\\n--- Final Response: ---\")\n",
    "        print(response)\n",
    "        if webpages:\n",
    "            print(\"\\n--- Scraped Webpages: ---\")\n",
    "            for page in webpages:\n",
    "                print(f\"- {page['question']}: {page['url']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
