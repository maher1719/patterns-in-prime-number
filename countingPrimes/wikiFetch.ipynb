{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from functools import lru_cache\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "\n",
    "# Caching page info results to avoid repeated requests\n",
    "@lru_cache(maxsize=1000)\n",
    "def fetch_wikipedia_page_info(title):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"info\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"inprop\": \"url|id\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    page_info = {}\n",
    "    for page_id, page in pages.items():\n",
    "        page_info['id'] = page.get('pageid')  # Using .get() to avoid KeyError\n",
    "        page_info['title'] = page.get('title')\n",
    "        page_info['url'] = f\"https://en.wikipedia.org/?curid={page_info['id']}\"\n",
    "    return page_info\n",
    "\n",
    "# Asynchronous functions\n",
    "async def fetch_wikipedia_page_info_async(session, title):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"info\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"inprop\": \"url|id\"\n",
    "    }\n",
    "    async with session.get(url, params=params) as response:\n",
    "        data = await response.json()\n",
    "        pages = data['query']['pages']\n",
    "        page_info = {}\n",
    "        for page_id, page in pages.items():\n",
    "            page_info['id'] = page.get('pageid')  # Using .get() to avoid KeyError\n",
    "            page_info['title'] = page.get('title')\n",
    "            page_info['url'] = f\"https://en.wikipedia.org/?curid={page_info['id']}\" if page_info.get('id') else None\n",
    "    return page_info\n",
    "\n",
    "async def fetch_wikipedia_links_async(session, title):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"links\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"pllimit\": \"max\"\n",
    "    }\n",
    "    async with session.get(url, params=params) as response:\n",
    "        data = await response.json()\n",
    "        pages = data['query']['pages']\n",
    "        links = []\n",
    "        for page_id, page in pages.items():\n",
    "            link_list = page.get('links', [])\n",
    "            for link in link_list:\n",
    "                links.append({'title': link['title']})\n",
    "    return links\n",
    "\n",
    "# Async batch fetching of page IDs using aiohttp\n",
    "async def fetch_all_link_ids_async(links):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_wikipedia_page_info_async(session, link['title']) for link in links]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)  # Ensuring all coroutines are awaited properly\n",
    "    return results\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(base_page_info, links_with_ids):\n",
    "    base_title = base_page_info['title']\n",
    "    filename = f\"{base_title.replace(' ', '_')}.csv\"\n",
    "\n",
    "    data = []\n",
    "    for link in links_with_ids:\n",
    "        data.append([\n",
    "            base_page_info['id'],\n",
    "            base_page_info['title'],\n",
    "            base_page_info['url'],\n",
    "            link.get('title'),  # Using .get() to avoid errors\n",
    "            link.get('id', None)  # Get the page ID if available\n",
    "        ])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Base Page ID', 'Base Page Title', 'Base Page URL', 'Link Title', 'Link Page ID'])\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Main function for running async tasks in Jupyter\n",
    "async def main():\n",
    "    base_page_title = \"Mathematics\"\n",
    "\n",
    "    # Fetch base page info\n",
    "    base_page_info = fetch_wikipedia_page_info(base_page_title)\n",
    "\n",
    "    # Asynchronously fetch links and link IDs\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        links = await fetch_wikipedia_links_async(session, base_page_title)\n",
    "        links_with_ids = await fetch_all_link_ids_async(links)\n",
    "\n",
    "    # Save to CSV\n",
    "    save_to_csv(base_page_info, links_with_ids)\n",
    "    print()\n",
    "\n",
    "# Avoid \"asyncio.run()\" in Jupyter notebooks, use nest_asyncio instead\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    await main()  # Using await instead of asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
